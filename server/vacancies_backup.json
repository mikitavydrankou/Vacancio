[
  {
    "company": "nineDots.io",
    "position": "Senior DevOps Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "CI/CD",
      "GitHub Actions",
      "Bitbucket",
      "TeamCity",
      "JFrog",
      "Python",
      "Java",
      "Guidewire",
      "HashiCorp Vault",
      "Datadog",
      "AWS"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "You’ll be designing and building an enterprise-grade CI/CD integration with Guidewire.",
      "Real ownership from day one.",
      "Architecture, hands-on build, pipeline control, repo sync, quality gates, secrets, observability."
    ],
    "requirements": [
      "You’ll need strong CI/CD experience.",
      "At this level, you will need over 7 years of experience."
    ],
    "work_mode": "remote",
    "employment_type": "contract",
    "seniority": "Seniority.senior",
    "description": "We’re hiring for a client in Dublin who are genuinely one of the good ones. They treat their contractors like adults, include them properly, and don’t play the usual games.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4343778133&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&start=25"
  },
  {
    "company": "OEC",
    "position": "DevOps Engineer (Data Platform)",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Python",
      ".NET",
      "SQL",
      "Terraform",
      "Jenkins",
      "Datadog",
      "Jira",
      "Bitbucket"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Optimize AWS resources for performance, cost-efficiency, and security.",
      "Lead and support migrations from on-premises to AWS-based data platforms.",
      "Implement and maintain DevOps practices such as CI/CD pipelines, test automation, and infrastructure as code (Terraform).",
      "Mentor and support a team of data engineers, while staying hands-on in pipeline development and architecture design.",
      "Build scalable, serverless, event-driven ETL pipelines to process and transform large datasets from diverse sources.",
      "Contribute to the planning and design of future data platform development phases."
    ],
    "requirements": [
      "Have 3+ years of professional experience as a DevOps or Platform Engineer with strong focus on AWS services (e.g., S3, Lambda, RDS, Glue and IAM).",
      "Are proficient in Python (or .NET/other relevant languages used in serverless environments).",
      "Are familiar with data ingestion mechanisms such as APIs, SFTP, message queues, or streaming platforms.",
      "Have experience building event-driven architectures and scalable, cloud-native data pipelines.",
      "Care about data quality, testing and graceful error handling.",
      "Use observability tools to monitor logs, performance, and system health (e.g., Datadog).",
      "Are comfortable building and managing infrastructure using Terraform."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Join us in transforming how vehicles are diagnosed and repaired — faster, smarter, and at scale. As a Data Platform Engineer, you'll lead the design and evolution of our cloud-native data platform, powering intelligent automotive services with cutting-edge data solutions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4351975745&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&start=25"
  },
  {
    "company": "Net2Source (N2S)",
    "position": "SaaS Operations Engineer",
    "location": "Wroclaw",
    "salary": "60000 - 100000 PLN",
    "tech_stack": [
      "JavaScript",
      "PowerShell",
      "Bash",
      "SQL",
      "JobScheduler",
      "Jenkins",
      "Nageos",
      "Centreon",
      "ELK",
      "Ansible",
      "Terraform",
      "Azure pipelines",
      "Docker",
      "Kubernetes",
      "AWS",
      "Azure",
      "PostgreSQL",
      "Oracle",
      "SQL Server",
      "Windows",
      "Linux"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Prepare scripts, deploy and update the application in different environments.",
      "Perform the integration test to ensure that all services are working as intended.",
      "Monitor and guarantee the quality of services provided to clients.",
      "Participate in on-call rotation to respond to critical incidents on Production."
    ],
    "requirements": [
      "A person with experience and willing to develop his skills in SaaS solutions.",
      "Bachelor's or Master's degree in Computer Science or related fields."
    ],
    "work_mode": "onsite",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Net2Source (N2S) is a Minority owned global workforce solutions company recognized by SIA as the largest and fastest-growing Total Talent Solutions provider with a presence in 34 countries. The company specializes in providing custom talent solutions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4354254631&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&start=25"
  },
  {
    "company": "PAPAYA",
    "position": "Senior Devops Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Kubernetes",
      "ArgoRollouts",
      "NodeJs",
      "TypeScript",
      "Python",
      "CI/CD",
      "Terraform",
      "Helm"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Build a high-scale backend platform for millions of users.",
      "Lead end-to-end solutions from requirements to production with a design that decreases the time-to-market.",
      "Write high-quality well architectured systems following industry best practices and standards.",
      "Experiment with new technologies, tools, and 3rd party frameworks. Utilize them to deliver state-of-the-art tech.",
      "Mentor new members of the team."
    ],
    "requirements": [
      "4+ years as DevOps Engineer (or equal role).",
      "B.Sc. in Computer Science or related field - Advantage.",
      "Hands-on experience in cloud architecture and relevant tools.",
      "Proficiency in a major development/scripting language.",
      "Autodidactic learner, innovative, passionate for learning new technologies.",
      "Familiar with best practices, and industry trends.",
      "Can-do approach.",
      "Excellent communication skills in English (verbal and written)."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "At Papaya, we believe in the power of play. Our mission is to create engaging, high-quality mobile gaming experiences that entertain millions of players worldwide. We specialize in skill-based games that combine fun, competition, and real rewards, fostering a strong sense of community and engagement.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4303579907&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&start=25"
  },
  {
    "company": "Stefanini EMEA",
    "position": "Sr. Cloud DevOps Engineer (AWS/Azure)",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Azure",
      "CI/CD",
      "GitHub",
      "Terraform",
      "PowerShell",
      "ARM Templates"
    ],
    "nice_to_have_stack": [
      "Docker",
      "Kubernetes",
      "Prometheus",
      "Grafana",
      "ELK stack"
    ],
    "responsibilities": [
      "Design, implement, and manage CI/CD pipelines using GitHub Actions or similar tools.",
      "Drive automation through advanced scripting (PowerShell, ARM Templates, etc.) and infrastructure as code tools like Terraform.",
      "Manage and support both Azure and AWS cloud environments. Support deploying Azure IaaS, PaaS, and SaaS solutions, ensuring compliance with industry best practices and organizational standards.",
      "Work closely with development teams to integrate infrastructure builds with application deployment processes.",
      "Automate operational processes as much as possible, adhering to the principles of Infrastructure as Code (IaC).",
      "Conduct regular reviews and audits of Azure/AWS environments to ensure ongoing operational excellence.",
      "Monitor and improve system performance, reliability, and scalability.",
      "Ensure security best practices are implemented and maintained across all cloud services.",
      "Troubleshoot and resolve complex infrastructure issues.",
      "Stay updated with emerging technologies and industry trends and apply this knowledge to improve our customer's infrastructure.",
      "Provide expertise on Azure networking components including Network Security Groups (NSGs), Application Security Groups (ASGs), load balancers, and application gateways."
    ],
    "requirements": [
      "Bachelor's degree in computer science, Information Technology, Engineering, or a related field.",
      "Minimum of 6-8 years of experience in a DevOps Engineer/ Cloud Engineer role or similar.",
      "Strong knowledge of AWS and Azure services and management.",
      "Proficiency in CI/CD tools, preferably GitHub Actions.",
      "Experience with infrastructure automation using Terraform/Bicep.",
      "Familiarity with containerization and orchestration technologies (e.g., Docker, Kubernetes).",
      "Solid understanding of networking, security, and database concepts.",
      "Experience with scripting languages such as Python, Bash, or PowerShell.",
      "Experience with Azure networking and security, particularly Hub/Spoke topology, NSGs, ASGs, and Load Balancers.",
      "Excellent problem-solving skills and the ability to work under pressure.",
      "Strong communication and collaboration skills."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Stefanini Group is looking for a Sr. Cloud DevOps Engineer to complement the technical capabilities of our Hybrid Cloud Infrastructure Tower (HCI) within the Infrastructure Services Division.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4362486087&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "The Stepstone Group Polska",
    "position": "Staff Dev Ops Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Terraform",
      "Linux",
      "Python"
    ],
    "nice_to_have_stack": [
      "Bash",
      "Go"
    ],
    "responsibilities": [
      "Establish governance policies and implement security controls for our tools and cloud resources.",
      "Implement infrastructure-as-code using Terraform, ensuring reproducible, version-controlled environments.",
      "Automate environment provisioning, configuration, and maintenance tasks.",
      "Ensure the stability, availability, performance, and cost-efficiency of our environments.",
      "Design and implement comprehensive monitoring, alerting, and observability solutions including cost anomaly detection and security monitoring."
    ],
    "requirements": [
      "5+ years of professional experience in DevOps, Platform Engineering, or Site Reliability Engineering roles.",
      "AWS certification (Solutions Architect, DevOps Engineer, SysOps Administrator, or similar) with hands-on experience across EC2, S3, RDS, Route53, CloudFront, and ECS/EKS.",
      "Infrastructure-as-Code expertise with Terraform, CloudFormation, or similar tools.",
      "Linux/Unix systems administration with strong command-line skills and scripting abilities (Python preferred; Bash, Go, or similar).",
      "Network architecture design and implementation experience including VPCs, security groups, and network ACLs."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for jobseekers and companies around the world to find a perfect match in fair and equitable way. Join our Platform Engineering team and play a critical role in modernizing and optimizing our development infrastructure.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4349734259&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Strategic Staffing Solutions",
    "position": "Cyber Security Engineer",
    "location": "Warsaw",
    "salary": "200+ PLN",
    "tech_stack": [
      "AWS",
      "AI",
      "ML",
      "web applications"
    ],
    "nice_to_have_stack": [
      "Kubernetes",
      "Azure",
      "GCP",
      "Terraform",
      "CloudFormation",
      "AWS CDK"
    ],
    "responsibilities": [
      "Conduct cybersecurity design and architecture reviews, including AI and ML solutions.",
      "Advise engineering teams on secure design and implementation, with a focus on web applications and AWS.",
      "Create, document, and promote secure design patterns and best practices.",
      "Lead risk read-out calls with the business and recommend mitigation strategies.",
      "Review penetration test and code analysis findings and guide teams through remediation.",
      "Mentor and support junior team members."
    ],
    "requirements": [
      "4+ years of experience in application security and/or cloud security.",
      "Experience with threat modeling or secure design and architecture reviews.",
      "Degree in Computer Science, Engineering, Cybersecurity, or Information Security.",
      "Strong knowledge of common vulnerabilities (OWASP Top 10, cloud security gaps).",
      "Hands-on experience with AWS security services (IAM, KMS, CloudTrail, GuardDuty, Inspector).",
      "Knowledge of authentication and authorization protocols (OAuth, OIDC, SAML).",
      "Understanding of secure coding practices and security controls.",
      "Experience with vulnerability assessment and penetration testing tools.",
      "Familiarity with modern web technologies and stacks.",
      "Knowledge of cryptography concepts such as TLS, encryption, and hashing.",
      "Strong English communication skills.",
      "Ongoing interest in learning about emerging security threats."
    ],
    "work_mode": "onsite",
    "employment_type": "contract",
    "seniority": null,
    "description": "Our customer, Goldman Sachs, protects its technology landscape through a global Technology Risk organization led by the Chief Information Security Officer (CISO). The Technology Risk team operates globally across the Americas, APAC, India, and EMEA, focusing on securing applications and infrastructure, preventing cyber threats, measuring risk, and designing strong cybersecurity controls.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4363373146&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Capgemini",
    "position": "Cloud Operation Engineer",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "Azure",
      "AWS",
      "GCP",
      "Puppet",
      "Ansible",
      "Chef",
      "OpenStack",
      "GitHub",
      "Bitbucket",
      "GitLab",
      "Bamboo",
      "Jenkins",
      "Python",
      "Bash",
      "Perl",
      "PowerShell",
      "Terraform"
    ],
    "nice_to_have_stack": [
      "Docker",
      "Kubernetes",
      "Agile",
      "Lean",
      "Linux",
      "Windows"
    ],
    "responsibilities": [
      "Provide top-level technology consultancy services to our clients.",
      "Support implementation programs for complex enterprise-scale solutions.",
      "Work with Architects, Subject Matter Experts, Solution Designers on client site."
    ],
    "requirements": [
      "Experience in large scale IT environments.",
      "Experience working with Azure or/and AWS or/and GCP cloud technologies.",
      "English or/and German language skills minimum on B2 level.",
      "Acting as an engineer on minimum 1 Cloud-based solution (Azure, AWS, Google, Oracle ect.).",
      "Experience in provisioning automation and configuration using minimum 1 of following: Puppet, Ansible, Chef, OpenStack.",
      "Experience with version control tools (GitHub, Bitbucket, GitLab).",
      "CI/CD tools (e.g. Bamboo, Jenkins).",
      "Proficiency in at least one of scripting language (e.g. Python, Bash, Perl, PowerShell etc.).",
      "Strong understanding of network infrastructure and security concepts (e.g. Terraform, Biceps, etc.)."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Capgemini is an AI-powered global business and technology transformation partner, delivering tangible business value. We imagine the future of organizations and make it real with AI, technology and people.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4360965048&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "European Tech Recruit",
    "position": "Senior DevOps Engineer - Kubernetes / Kafka / AWS",
    "location": "Remote",
    "salary": "70000 - 100000 EUR",
    "tech_stack": [
      "Kubernetes",
      "AWS",
      "Terraform"
    ],
    "nice_to_have_stack": [
      "KAFKA"
    ],
    "responsibilities": [],
    "requirements": [
      "5+ years in DevOps Engineering",
      "Experienced with Kubernetes and Terraform",
      "Knowledge of Kafka would be nice to have",
      "AWS would also be great"
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "We're hiring for an experienced Senior DevOps Engineer with experience in Kubernetes for a rapidly expanding and well established software company HQ'd in Switzerland.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4362494493&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "CGI",
    "position": "DevOps Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "Azure Cloud",
      "MS SQL Server",
      "Power BI",
      "Zabbix",
      "ELK",
      "Ansible",
      "Terraform",
      "GitHub",
      "Satellite",
      "HashiCorp Vault",
      "Azure DevOps"
    ],
    "nice_to_have_stack": [
      "VMware"
    ],
    "responsibilities": [
      "Budowę, uruchamianie, konfiguracje i administracje środowiskami projektowymi, on-premise i Azure Cloud.",
      "Dbanie o bezpieczeństwo środowisk.",
      "Wsparcie innych zespołów w tym analityków, developerów, testerów.",
      "Współpracę z zespołami Klienta w zakresie administracji środowisk oraz z dostawcami i podwykonawcami infrastruktury (serwery i macierze On-Premise).",
      "Przygotowanie instrukcji i dokumentacji dla Klienta w celu przekazania wiedzy."
    ],
    "requirements": [
      "Doświadczenie na podobnym stanowisku i pracy w zespołach DevOps w złożonych projektach wdrożeniowych lub/i integracyjnych.",
      "Znajomość języka angielskiego pozwalająca na codzienną komunikację w ramach projektu.",
      "Znajomość zarządzania usługami w publicznej chmurze obliczeniowej – preferowana Azure Cloud.",
      "Doświadczenie w budowie i utrzymaniu środowisk, rozwoju systemów budowanych w modelu CI/CD."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Dołączając do naszego zespołu specjalistów, będziesz brać udział w prestiżowym projekcie wdrożeniowym naszego produktu, wspomagającego transformację branży elektroenergetycznej. Początkowa faza projektu będzie dotyczyła środowisk chmurowych, w kolejnych fazach będą uruchamiane rozwiązania On-Premise.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4255148135&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Vonage",
    "position": "Platform Engineer",
    "location": "Wroclaw",
    "salary": "",
    "tech_stack": [
      "Kubernetes",
      "Python",
      "Go",
      "Terraform",
      "Crossplane",
      "AWS",
      "GCP"
    ],
    "nice_to_have_stack": [
      "Argo",
      "Helm",
      "Ansible",
      "CNCF tooling",
      "VictoriaMetrics",
      "Thanos",
      "Cortex",
      "Prometheus",
      "Datalog",
      "Grafana"
    ],
    "responsibilities": [
      "In this position you’ll actively seek out ways to contribute and bring new ideas.",
      "You will have the opportunity to extensively utilize open source projects/software, often focusing on developing the 'glue' that integrates systems and enhancing their feature sets.",
      "Bringing a sense of vision and contributing to the requirements is expected and encouraged."
    ],
    "requirements": [
      "3+ years of progressive experience in a combination of development, design in areas of cloud computing.",
      "Coding skills in modern language, ideally Python or Go and SDLC management.",
      "Automate Kubernetes infrastructure deployment and management using tools such as Terraform, Crossplane to develop self-service platform workflows.",
      "Strong Infrastructure as a Code (IaC) experience on public Cloud (AWS and/or GCP), integrating Terraform/Crossplane to extend IaC capabilities across multiple cloud providers.",
      "Fluency with Cloud API SDK’s (AWS/GCP) and identify opportunities to deliver self service capabilities for the most common infrastructure and application management tasks.",
      "Experience with Unix/Linux OS internals (filesystems, system calls) and networking.",
      "Experience in working on a large scale, highly available, cloud native infrastructure platform on a public cloud, preferably in a consumer business.",
      "Experience with any K8s managed service (EKS/GKE) with connected automation and orchestration tool development experience such as Argo, Helm, Terraform, Ansible and CNCF tooling ecosystem."
    ],
    "work_mode": "onsite",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "Join Vonage and help us innovate cloud communications for businesses worldwide! On the Platform Engineering Team, you will be responsible for designing, implementing, and managing internal tool sets used by our Engineering teams in their daily routines.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4352920542&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "SmartBear",
    "position": "DevOps Engineer",
    "location": "Wroclaw",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Terraform",
      "Terragrunt",
      "Kubernetes",
      "Helm",
      "GitHub Actions",
      "Jenkins",
      "MySQL",
      "Postgres"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Work with Development, Product, and Support teams to build highly scalable and resilient systems.",
      "Collaborate with other operations teams in Poland, India, and the United States.",
      "Automate and implement scalable, highly available application deployments backed by IaC, and improve efficiency across cloud infrastructure.",
      "Maintain efficient CI/CD pipelines in GitHub Actions to support build and deployment automations for various web applications.",
      "Leverage AI tools to increase automation and improve observability across production systems.",
      "Implement automation tools and scripts as necessary to evolve systems and processes that assist various development and support teams.",
      "Document implemented systems and processes across internal and external applications.",
      "Participate in on-call rotations that support the global nature of SmartBear's product portfolio."
    ],
    "requirements": [
      "3-5 years of experience in a DevOps, System Admin, Operations, or similar role.",
      "Experience working with cloud infrastructure, notably AWS.",
      "Experience with Infrastructure as Code, particularly Terraform or Terragrunt.",
      "Experience and confidence with orchestration tools, especially Kubernetes and Helm.",
      "Experience with CI/CD workflows, like GitHub Actions or Jenkins.",
      "Familiarity with database administration, especially MySQL and Postgres.",
      "Familiarity with networking concepts, especially load balancing, reverse proxying, and tunneling.",
      "A good understanding of Linux/Unix fundamentals and administration.",
      "A good understanding of git, GitHub, and source code management methodologies."
    ],
    "work_mode": "onsite",
    "employment_type": "full-time",
    "seniority": null,
    "description": "At SmartBear, we deliver the complete visibility developers need to make each release better than the last. Our award-winning and industry-favorite tools—TestComplete, Swagger, Cucumber, ReadyAPI, and Zephyr—are trusted by over 16 million developers, testers, and software engineers at 32,000+ organizations, including world-renowned innovators like Adobe, JetBlue, FedEx, and Microsoft.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4325755818&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Octopus Deploy",
    "position": "Senior DevOps Engineer - EMEA",
    "location": "Remote",
    "salary": "60000 - 65000 USD",
    "tech_stack": [
      "Kubernetes",
      "GitOps",
      "AWS",
      "GCP",
      "Azure",
      "Terraform",
      "Pulumi",
      "Prometheus",
      "Grafana",
      "Loki",
      "OpenTelemetry",
      "Bash",
      "Python"
    ],
    "nice_to_have_stack": [
      "Crossplane",
      "Argo CD",
      "Argo Workflows",
      "RabbitMQ",
      "MongoDB",
      "NoSQL",
      "Go",
      "Node.js"
    ],
    "responsibilities": [
      "Your core mission will be to foster a culture of collaboration, shared ownership, and excellence.",
      "You'll work closely with development and product teams to streamline their workflows, champion best practices, and mentor engineers."
    ],
    "requirements": [
      "5+ years of experience with DevOps and cloud technologies.",
      "Deep expertise in Kubernetes and containerization.",
      "Strong knowledge of at least one major public cloud (AWS, GCP, or Azure); experience with more than one is a plus.",
      "Proficiency with Infrastructure as Code (Terraform, Pulumi, or similar).",
      "Strong familiarity with CI/CD practices and tooling.",
      "Experience with monitoring and observability stacks (Prometheus, Grafana, Loki, OpenTelemetry).",
      "Strong scripting and automation skills (Bash, Python).",
      "Excellent problem-solving and collaboration skills in distributed, cross-functional teams."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Octopus Deploy sets the standard for Continuous Delivery, empowering software teams to deliver value in an agile way. The DevOps team is integral to shaping the direction of the product.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4329185803&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Turnitin",
    "position": "DevOps Engineer",
    "location": "Remote",
    "salary": "119250 - 198750 PLN",
    "tech_stack": [
      "AWS",
      "Terraform",
      "Ansible",
      "Docker",
      "Kubernetes",
      "Jenkins",
      "GitHub Actions",
      "CloudFormation",
      "CloudWatch",
      "Grafana",
      "Prometheus",
      "OpenSearch"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Collaborate cross-functionally with the Engineering, Quality Assurance, and Support teams.",
      "Help break down projects and features into smaller, manageable tasks for the team to work on.",
      "Assist with setting up and maintaining infrastructure in both on-premises and AWS environments, following established designs and guidance from senior team members.",
      "Contribute readable, testable, maintainable & documented code when making changes to our infrastructure through Infrastructure as Code (IaC) systems like Terraform, or AWS Cloudformation.",
      "Contribute readable, testable, maintainable & documented code when managing configuration for our infrastructure through Configuration as Code (IaC) systems like Ansible, or Puppet.",
      "Ensure systems and platforms relied upon by both external and internal customers are fault-tolerant, highly available."
    ],
    "requirements": [
      "Familiarity with AWS Serverless technologies and interest in learning and growing in that area.",
      "Working knowledge of containerization technology (Docker) and administration of distributed containerization orchestration like Kubernetes (including EKS/AKS/GKE) or ECS.",
      "Experience with Infrastructure as Code tools such as Terraform, AWS CDK, or CloudFormation.",
      "Demonstrable experience and curiosity when troubleshooting full-stack production systems (including network, storage, compute layers, and service dependencies such as DNS, DB, etc.).",
      "Experience with continuous integration and delivery platforms such as Jenkins, Github Actions, or Bitbucket Pipelines.",
      "A fundamental understanding of microservices, micro front-ends and distributed architecture.",
      "Knowledge of monitoring and logging in cloud-native environments, including CloudWatch, Grafana, Prometheus, and OpenSearch."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Turnitin is a recognized innovator in the global education space. For over 25 years, Turnitin has partnered with educational institutions to promote honesty, consistency, and fairness across all subject areas and assessment types, impacting over 21,000 academic institutions, publishers, and corporations.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4358295554&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Samba TV",
    "position": "Senior Cloud Engineer",
    "location": "Warsaw",
    "salary": "290000 - 350000 PLN",
    "tech_stack": [
      "AWS",
      "GCP",
      "Kubernetes",
      "Terraform",
      "Docker",
      "Python",
      "Bash"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Lead the architecture, design, and implementation of scalable, secure, and highly available cloud platforms (e.g., AWS, GCP).",
      "Architect, deploy, and manage production-grade Kubernetes clusters supporting mission-critical, containerized applications.",
      "Define and enforce cloud standards, patterns, and best practices across the organization.",
      "Maintain, monitor, and optimize cloud environments, including compute, storage, networking, and security services.",
      "Troubleshoot complex cloud-related issues and provide timely resolutions to ensure high system availability and performance.",
      "Design and maintain robust Infrastructure as Code (IaC) frameworks using tools such as Terraform, ensuring repeatability and compliance.",
      "Establish and evolve CI/CD pipelines to support secure, reliable, and scalable application delivery.",
      "Drive automation-first approaches to infrastructure provisioning, configuration management, and operational workflows.",
      "Implement and manage containerization technologies such as Docker and orchestration platforms, with a strong emphasis on Kubernetes.",
      "Provide technical leadership for cloud operations, including monitoring, logging, alerting, backup, and disaster recovery strategies.",
      "Design and enforce cloud security best practices, including identity and access management (IAM), network segmentation, encryption, and compliance controls.",
      "Own cloud cost management strategies, including cost optimization, budgeting, forecasting, and resource governance.",
      "Serve as a subject matter expert for cloud technologies, providing guidance and support across Engineering, Product, Operations, and Security teams.",
      "Collaborate with other departments such as Development, Product, and Security to translate business requirements into scalable and resilient cloud solutions.",
      "Lead documentation efforts for cloud architectures, operational runbooks, and best practices."
    ],
    "requirements": [
      "Bachelor's degree in Computer Science, a related field, or equivalent practical experience.",
      "6+ years of experience in cloud engineering, DevOps, or cloud infrastructure roles, with demonstrated senior-level responsibility.",
      "Deep hands-on experience with at least one major cloud platform (AWS or GCP), including production-scale environments.",
      "Expert-level experience with Infrastructure as Code (e.g., Terraform) and infrastructure automation.",
      "Extensive experience with containerization technologies such as Docker and Kubernetes ecosystems, including designing, deploying, managing, and troubleshooting clusters at scale.",
      "Proficiency in scripting and automation (e.g., Python, Bash).",
      "Strong understanding of cloud security principles, including IAM, network security, encryption and compliance.",
      "Strong problem-solving skills with the ability to work independently and manage multiple projects simultaneously.",
      "Excellent communication skills, with the ability to explain complex technical concepts to technical and non-technical stakeholders."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Samba TV tracks streaming and broadcast video across the world with proprietary data and technology to transform the viewing experience. They enable media companies to connect with audiences for new shows and movies, and advertisers to engage viewers and measure reach across devices.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4346658561&f_AL=true&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R"
  },
  {
    "company": "Cenosco",
    "position": "DevOps Team Lead",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Docker",
      "Kubernetes",
      "Terraform"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Lead, coach, and develop the Infrastructure & DevOps team, fostering ownership and collaboration.",
      "Define and own the Infra & DevOps roadmap, standards, and ways of working.",
      "Align stakeholders and drive cross-team collaboration across engineering, product, and security.",
      "Drive containerization and orchestration strategies with Docker and Kubernetes.",
      "Guide the adoption of Infrastructure as Code (Terraform and related tools).",
      "Oversee delivery of reliable, scalable, and secure platform infrastructure.",
      "Mentor DevOps engineers and foster a culture of continuous improvement.",
      "Remove impediments, manage risks, and ensure predictable, high-quality outcomes.",
      "Champion continuous improvement in performance, security, and operational efficiency."
    ],
    "requirements": [
      "10+ years of experience in software engineering or infrastructure, including 6+ years in DevOps.",
      "2+ years in a people leadership or team lead role.",
      "Strong understanding of CI/CD, cloud infrastructure, and container platforms (Docker, Kubernetes).",
      "Experience with Infrastructure as Code and modern deployment strategies.",
      "Proven ability to lead teams, drive technical initiatives, and influence without micromanaging.",
      "Familiarity with security and compliance in DevOps workflows.",
      "Excellent communication, stakeholder management, and organizational skills."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.lead",
    "description": "At Cenosco, we develop cutting-edge Asset Integrity Management Software used by global industry leaders. As the Infra & DevOps Team Lead, you will be responsible for team leadership, delivery ownership, and continuous improvement of our infrastructure practices that support our next-generation SaaS platform.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4363654630&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "PRACYVA",
    "position": "Infrastructure Project Manager",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Cloud",
      "Virtualization",
      "Database",
      "Networking",
      "Azure Cloud"
    ],
    "nice_to_have_stack": [
      "ADO",
      "Agile Management",
      "PRINCE2",
      "PMP"
    ],
    "responsibilities": [
      "Managing all large-scale multiple Cloud transformation & Migrations projects within cloud practice.",
      "Preparing/tracking & managing the Project schedules.",
      "Executing large scale Cloud Migrations, Integrations and Transformations.",
      "Driving major IT projects, managing multiple priorities simultaneously, setting timelines, holding people accountable and delivering projects on-time and on-budget.",
      "Planning and managing multiple stakeholders including client, offshore and cross functional teams to deliver the transformations required.",
      "Managing Governance, collating projects status & reporting at regular intervals to leadership team.",
      "Tracking & reporting schedule and cost variance across projects.",
      "Identifying issues, risks and track till closure or mitigation/resolution of all cloud transformation projects."
    ],
    "requirements": [
      "Overall 12+ years of cloud transformational & consulting projects migration experience.",
      "Educational Qualification: BCA/MCA/BE/B.Tech or equivalent."
    ],
    "work_mode": "remote",
    "employment_type": "contract",
    "seniority": "Seniority.mid",
    "description": "At Pracyva, we specialize in building future-ready teams for clients worldwide. Our recruitment services deliver digitally enabled, multidisciplinary workforce solutions to clients across the globe, bridging skilled professionals through extensive talent networks to shape the future.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4354203596&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "Symphony Solutions",
    "position": "Lead DevOps Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Docker",
      "Kubernetes",
      "Terraform",
      "CI/CD",
      "GitHub Actions",
      "GitLab",
      "Bitbucket"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Provide technical leadership, ensuring efficient coordination and execution of cloud infrastructure projects.",
      "Act as the primary technical contact for clients, leading refinement and planning sessions to align solutions with business needs.",
      "Design, deploy, and manage AWS cloud infrastructure.",
      "Work extensively with AWS core services.",
      "Implement Infrastructure as Code.",
      "Develop and maintain CI/CD pipelines.",
      "Build and manage containerized applications.",
      "Ensure high availability, scalability, and security of cloud environments.",
      "Troubleshoot complex infrastructure issues and provide use-case-specific architectural solutions.",
      "Collaborate effectively with cross-functional teams, sharing knowledge and driving cloud best practices."
    ],
    "requirements": [
      "Valid AWS Professional/Specialty Certification is required.",
      "4+ years of hands-on experience in designing, deploying, and managing cloud infrastructure using AWS.",
      "In-depth knowledge of standard AWS services such as VPC, EC2, S3, RDS, Lambda, ECS, IAM, SNS, SQS, API Gateway, EKS, CloudFront, EFS/EBS, SecurityHUB, Guard Duty, CloudWatch, AWS Application Migration Service, KMS, PCA, and similar.",
      "3+ years of experience in at least one Infrastructure as a Code technology like Terraform (preferred), AWS CloudFormation, Ansible, AWS CDK, or similar.",
      "3+ years of experience in building and managing CI/CD pipelines (GitHub Actions, GitLab, Bitbucket, or similar).",
      "2+ years of experience in creating and running container-based applications using Docker and Kubernetes/EKS.",
      "At least a bachelor’s degree in Computer Science, Information Technology, or a related field.",
      "Strong understanding of networking concepts (TCP/IP, DNS, load balancers, VPN) and related AWS Services (CloudFront, API Gateway, VPC, Route 53, ELB).",
      "Strong understanding of security concepts (IAM, Access Control, Endpoint Security, Data Encryption, firewalls).",
      "Excellent problem-solving skills for use-case-specific architecture design and implementation.",
      "Strong communication and collaboration skills with the ability to work effectively in a team environment."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.lead",
    "description": "We are looking for a talented and motivated Lead DevOps Engineer to join our Managed Services team. This is an exciting opportunity to work across various projects and domains. As part of our team, you will be providing DevOps services to a range of projects, not tied to just one. The dynamic nature of this role means you will need to adapt to different environments, technologies, and business needs.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4364263283&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "Kingfisher plc",
    "position": "Cloud Engineer (DevSecOps)",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "AWS",
      "GCP",
      "DevSecOps",
      "CI/CD",
      "GitLab",
      "Terraform",
      "Python",
      "Shell",
      "Bash"
    ],
    "nice_to_have_stack": [
      "Palo Alto security tools",
      "Datadog",
      "Dynatrace"
    ],
    "responsibilities": [
      "Embed security best practices into all stages of the software delivery lifecycle.",
      "Drive automation in infrastructure, CI/CD pipelines, and security controls.",
      "Support the design and delivery of secure pipelines and paths to production.",
      "Working closely with the Tech Lead, Product Owner and scrum masters to create, deliver and support the platform in line with our infrastructure strategy.",
      "Reviewing automation in technical areas and recommend improvements.",
      "Deploy, plan and maintain IAAS, PAAS & tooling capabilities.",
      "Creation and ongoing ownership of the platform common standards and practices."
    ],
    "requirements": [
      "Extensive experience in DevOps and Agile/Scrum methodologies.",
      "Strong background and working knowledge of DevSecOps practices.",
      "Experience in Continuous Delivery and collaboration with cross-functional teams.",
      "Customer-focused mindset and strong problem-solving skills for decision-making.",
      "Solid experience in build and release management with CI/CD automation tools (GitLab).",
      "Experience working with Jira or a similar tool.",
      "Proficiency working with Terraform, including managing secure infrastructure as code.",
      "Experience in Continuous Delivery, coding (Python, Shell, Bash), and a collaborative approach with cross-functional teams.",
      "Working experience with AWS and GCP, with strong expertise in at least one.",
      "Understanding of networking concepts, technologies, and protocols: TCP/IP, IPSec, HTTPS, SFTP, DHCP, DNS, TLS/SSL, SSH, OAuth/OpenID Connect.",
      "Familiarity with Palo Alto security tools.",
      "Familiarity with monitoring and security observability tools such as DataDog, Dynatrace, or similar.",
      "Excellent command of English (written and spoken)."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": null,
    "description": "At Kingfisher, we are seeking a Cloud Engineer (DevSecOps) to enhance our cloud platform with a focus on security and automation. You will help design secure delivery pipelines, improve infrastructure on AWS and GCP, and ensure best practices are followed across the software lifecycle.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4354299528&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "Experis Poland",
    "position": "DevSecOps Engineer",
    "location": "Krakow",
    "salary": "140 - 150 PLN",
    "tech_stack": [
      "Groovy",
      "Python",
      "Jenkins",
      "SonarQube",
      "Sonatype IQ",
      "SAST"
    ],
    "nice_to_have_stack": [
      "Terraform",
      "Helm",
      "GCP",
      "AWS"
    ],
    "responsibilities": [
      "Design, develop, and maintain Groovy-based CI/CD pipeline steps including build, test, package, scan, and deploy.",
      "Extend and maintain Python tooling for SLSA provenance, SBOM generation, hash and digest accuracy, and security scan aggregation across tools such as SonarQube, Sonatype IQ, SAST, and container scanning.",
      "Optimize pipeline performance through parallel builds, caching strategies, dependency prefetching, and scope-reduced BOMs.",
      "Ensure artifact integrity by implementing correct SHA1/SHA256 mappings, reproducible build inputs, and robust evidence modeling.",
      "Refactor and modernize legacy scripts by removing global state, consolidating hashing logic, and standardizing templates.",
      "Define, document, and promote ci-config.yaml standards and usage patterns.",
      "Mentor engineers in secure pipeline development and software supply-chain best practices.",
      "Troubleshoot, resolve, and proactively prevent CI/CD pipeline incidents."
    ],
    "requirements": [
      "Minimum 7 years of overall engineering experience, including at least 3 years in CI/CD platform engineering or DevSecOps roles.",
      "Strong hands-on expertise with Jenkins and Groovy shared libraries.",
      "Advanced Python skills for automation, including JSON and YAML processing and tooling development.",
      "Deep understanding of Maven, NPM, and Python packaging ecosystems.",
      "Practical experience with supply-chain security concepts such as SLSA, CycloneDX SBOMs, and artifact digests.",
      "Experience integrating security and quality scanning tools including SonarQube, Sonatype IQ, container scanning, and SAST solutions.",
      "Proven ability to optimize build and pipeline performance through caching, parallelization, and dependency optimization.",
      "Awareness and understanding of compliance-related requirements in CI/CD and software delivery environments."
    ],
    "work_mode": "hybrid",
    "employment_type": "contract",
    "seniority": null,
    "description": "Experis is the global leader in IT professional resourcing and project-based solutions, accelerating organizations' growth by attracting, assessing, and placing specialized expertise in IT to deliver in-demand talent for mission-critical positions and projects.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4359312545&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "Veeam Software",
    "position": "Platform Engineering Manager",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "cloud-native",
      "IAC",
      "automation",
      "CI/CD",
      "GitHub Actions",
      "Pulumi",
      "Terraform"
    ],
    "nice_to_have_stack": [
      "Azure"
    ],
    "responsibilities": [
      "Define and communicate the vision and strategy for the team.",
      "Build and mentor a motivated and cohesive team; foster an inclusive and diverse environment; promote career development.",
      "Guide the design, implementation, and operation of cloud native platform systems.",
      "Promote operational excellence, reliability and security.",
      "Encourage the adoption of modern IAC, automation, CI/CD, and observability tools.",
      "Partner with engineering, product, security and SRE teams to ensure the platform aligns with business objectives.",
      "Actively participate in design discussions and code reviews.",
      "Promote cross team collaboration."
    ],
    "requirements": [
      "3+ years of experience leading engineering teams in the Platform, DevOps, or SRE domains.",
      "5+ years of hands-on experience as an engineer.",
      "Experience building and operating high-scale cloud-native applications.",
      "Experience promoting a culture of Operational excellence with SaaS products.",
      "Experience with public cloud infrastructure.",
      "Experience automating infrastructure through code using technologies such as Pulumi or Terraform.",
      "A breadth and depth of hands-on experience public cloud services."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.manager",
    "description": "Veeam is the #1 global market leader in data resilience, providing data resilience through data backup, recovery, portability, security, and intelligence. The mission of the Platform Engineering team is to provide a secure, reliable, and easy to use platform for the Veeam Data Cloud product.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4330618346&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start=25"
  },
  {
    "company": "Belvedere Recruitment",
    "position": "DevOps Engineer - Cloud",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "Linux",
      "Python",
      "C++",
      "Jenkins",
      "GCP",
      "Docker"
    ],
    "nice_to_have_stack": [
      "Apache Beam",
      "Numerical algorithms",
      "Alibaba Cloud",
      "CMake",
      "gRPC",
      "Protobuf"
    ],
    "responsibilities": [
      "Build and manage Jenkins jobs to deploy applications across cloud environments in multiple configurations.",
      "Maintain cross-platform C++ libraries and manage shared object builds across different operating systems.",
      "Support team members with technical infrastructure questions and troubleshooting.",
      "Manage cloud storage connectivity and configuration across platforms.",
      "Develop automation scripts using Python to streamline deployment and infrastructure tasks.",
      "Work with containerisation tools to standardise application environments."
    ],
    "requirements": [
      "Strong proficiency in Linux system administration and Python scripting.",
      "Experience building and maintaining C++ libraries and shared objects.",
      "Hands-on experience with Jenkins for continuous integration and deployment.",
      "Working knowledge of Google Cloud Platform (GCP) and cloud storage solutions.",
      "Familiarity with containerisation tools such as Docker.",
      "Understanding of software and hardware limitations and their impact on application performance.",
      "Experience with at least one Linux distribution (Ubuntu or Red Hat)."
    ],
    "work_mode": "hybrid",
    "employment_type": "contract",
    "seniority": null,
    "description": "Our client, a forward-thinking technology company based in Krakow, is seeking a DevOps Engineer to join their technical support team. If you have strong Linux and Python skills and want to work across cloud infrastructure, containerisation and application deployment, this contract role offers the opportunity to support a talented engineering team whilst developing your expertise across multiple technologies and platforms.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4364311515&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "SpotOn",
    "position": "DevSecOps Engineer",
    "location": "Krakow",
    "salary": "16500 - 20000 PLN",
    "tech_stack": [
      "AWS",
      "Terraform",
      "Okta",
      "Zscaler",
      "Python",
      "Go"
    ],
    "nice_to_have_stack": [
      "CloudWatch",
      "Grafana",
      "SIEM"
    ],
    "responsibilities": [
      "Design, implement, and maintain AWS security controls and infrastructure using Terraform.",
      "Manage AWS Identity & Access Management (IAM) policies, roles, and permissions for least privilege.",
      "Configure and monitor AWS-native security services (GuardDuty, Security Hub, WAF, CloudTrail).",
      "Build and manage Zscaler configurations as code to enforce Zero Trust principles.",
      "Build and manage Okta configurations as code for policies, groups, and application integrations.",
      "Detect and remediate cloud misconfigurations, ensuring compliance with frameworks such as SOC 2 and PCI DSS.",
      "Support incident response by providing visibility into AWS resources and logs."
    ],
    "requirements": [
      "Strong experience with AWS services (IAM, VPC, EC2, S3, Lambda, WAF).",
      "Experience writing and maintaining infrastructure-as-code for AWS and Okta (Terraform Okta provider or similar).",
      "Knowledge of cloud security best practices and compliance frameworks (SOC 2 and PCI).",
      "Familiarity with monitoring/logging (CloudWatch, Grafana, SIEM).",
      "Experience managing Zscaler policies as part of enterprise security architecture.",
      "Strong problem-solving skills and ability to collaborate with cross-functional teams.",
      "Experience with automation and scripting (Python, Go, or similar)."
    ],
    "work_mode": "onsite",
    "employment_type": "full-time",
    "seniority": null,
    "description": "SpotOn helps restaurants grow profits, streamline operations, and own their customer experience by providing them with tools like point-of-sale and AI-powered profit solutions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4321296432&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "LoopMe",
    "position": "InfoSec (DevSecOps) Engineer",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "GCP",
      "Kubernetes",
      "PostgreSQL",
      "ClickHouse",
      "Envoy",
      "KAFKA"
    ],
    "nice_to_have_stack": [
      "Bash",
      "Python",
      "PowerShell"
    ],
    "responsibilities": [
      "Develop and implement information security policies and protection procedures.",
      "Perform risk assessments, security audits, and threat analysis.",
      "Monitor and respond to security incidents and conduct investigations.",
      "Implement and maintain security tools such as SIEM, DLP, WAF and others.",
      "Integrate DevSecOps practices into development workflows (Secure SDLC, code reviews).",
      "Ensure compliance with security standards (ISO/IEC 27001, NIST, OWASP, CIS Controls).",
      "Provide cybersecurity awareness training to employees.",
      "Support secure architecture for platforms including GCP, Kubernetes, ClickHouse, Kafka, PostgreSQL, and Envoy.",
      "Conducting proof-of-concept for new security integrations and actively participating in security budget discussions with product stakeholders and upper management."
    ],
    "requirements": [
      "Experience in information security or related fields (both formal education and practical hands-on experience are considered).",
      "2+ years of hands-on experience in InfoSec/DevSecOps roles, preferably in a cloud environment (GCP, AWS, Azure).",
      "Strong understanding of network protocols (TCP/IP, DNS, HTTP/S, VPN).",
      "Hands-on experience securing infrastructure based on GCP, Kubernetes, ClickHouse, Kafka, PostgreSQL.",
      "Familiarity with SIEM systems, vulnerability management tools, IAM/SSO/MFA solutions (e.g., Okta, Azure AD).",
      "Incident response and forensics experience (IR, investigations).",
      "Solid understanding of security standards and frameworks: ISO/IEC 27001, NIST, OWASP, DevSecOps principles.",
      "Strong understanding of security principles, protocols, and standards (e.g., encryption, authentication, access control).",
      "Experience with security tools and technologies for monitoring and incident response.",
      "Proficiency in securing Kubernetes, PostgreSQL, ClickHouse, Envoy, Kafka, and related technologies."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "LoopMe is a global team of skilled engineers who develop and maintain real-time bidding platforms for leading advertisers worldwide. Leveraging technologies such as Java, Postgres, Clickhouse, Kafka, you will be instrumental in scaling performance, optimizing cloud infrastructure, and creating innovative features across our product portfolio.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4303628680&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Crypto.com",
    "position": "Blockchain DevOps Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "Blockchain",
      "Amazon",
      "Azure",
      "Google",
      "Kubernetes",
      "Terraform",
      "Docker",
      "Ansible",
      "Grafana",
      "Prometheus",
      "Fluentd",
      "Elasticsearch",
      "Kibana",
      "Linux",
      "Networking",
      "Firewall",
      "IDS/IPS"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Maintain and support blockchain infrastructure",
      "Review and propose configuration of blockchain software and network setup of nodes running the blockchain software",
      "Develop deep understanding of the Blockchain technology (specifically public Blockchain)",
      "Develop and share with others on security threats of various blockchain technology",
      "Define and maintain team continuous devops lifecycle and automation",
      "Design and maintain cloud computing workload, manage cloud security posture"
    ],
    "requirements": [
      "Degree in Computer Science or Engineering or similar discipline",
      "3 - 5 years working experiences in devops or system administration or related area, applicants with less experience will be considered for a Junior position",
      "Understand basic blockchain (a plus if has experience in deploying blockchain)",
      "Experience in deploying in various cloud (Amazon, Azure, Google)",
      "Experience in developing and deploying infrastructure as code (Kubernetes, Terraform, Docker, Ansible, etc)",
      "Experience in deploying and maintaining container infrastructures (Kubernetes, docker-compose, etc)",
      "Experience in building, deploying and maintaining monitoring tools (Grafana, Prometheus, etc)",
      "Experience in building, deploying and maintaining logging tools (Fluentd, ElasticSearch, Kibana, etc)",
      "Good understanding of traditional security ops areas of expertise: Linux, Networking, Firewall, IDS/IPS",
      "Able to troubleshoot and debug issues, and demonstrate a methodical approach to root cause analysis",
      "A team player",
      "Eager for constant learning from both success and failure, remaining open to change and continuous improvement",
      "Enthusiastic about the future of blockchain",
      "Curious and comfortable with ambiguity"
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "Crypto.com is a team to design, develop, maintain, and improve software for various ventures projects, i.e., projects that are adjacent to our core businesses and are bootstrapped fast with a lean team. You will be actively involved in the design of various components behind scalable applications, from frontend UI to backend infrastructure.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4311931440&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Social Discovery Group",
    "position": "DevOps Team Lead",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Kubernetes",
      "GitLab CI/CD",
      "Shell scripting",
      "Linux",
      "Terraform",
      "Ansible",
      "Grafana",
      "OpenSearch"
    ],
    "nice_to_have_stack": [
      "networking protocols",
      "Windows environments",
      "VMware",
      "ELK",
      "EFK"
    ],
    "responsibilities": [
      "Lead the modernization and optimization of CI/CD pipelines across diverse systems and legacy components.",
      "Introduce and implement industry-leading DevOps tools and practices, fostering a culture of automation and reliability.",
      "Oversee and support existing CI/CD workflows, ensuring high stability and performance.",
      "Manage and optimize ephemeral and pre-production environments to improve efficiency and reduce deployment friction.",
      "Collaborate closely with development teams to align DevOps solutions with product needs.",
      "Mentor and guide DevOps engineers, promoting best practices and continuous improvement."
    ],
    "requirements": [
      "5+ years of experience in DevOps, infrastructure, or system engineering roles.",
      "Expertise in Kubernetes — orchestrating and managing containerized applications.",
      "Strong experience with GitLab CI/CD — designing, maintaining, and optimizing pipelines.",
      "Proficiency in Shell scripting and Linux administration.",
      "Hands-on experience with Terraform and Ansible.",
      "Knowledge of observability tools such as Grafana and OpenSearch.",
      "Solid understanding of DevOps principles, CI/CD methodologies, and automation practices.",
      "Strong communication skills and the ability to lead, motivate, and mentor a team.",
      "Problem-solving mindset and persistence in achieving results."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.lead",
    "description": "Social Discovery Group (SDG) is the 3rd largest social discovery company in the world, uniting 60+ brands with 500 million users. They are looking for a DevOps Team Lead to join their engineering team and drive the transformation of CI/CD workflows across multiple complex products.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4352400313&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Arista Networks",
    "position": "Site Reliability Engineer (SRE) - CloudVision",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Kubernetes",
      "Spinnaker",
      "GKE",
      "HBase",
      "Hadoop",
      "Elasticsearch",
      "ClickHouse",
      "KAFKA",
      "TensorFlow",
      "Prometheus",
      "Grafana",
      "Loki"
    ],
    "nice_to_have_stack": [
      "PostgreSQL",
      "Docker",
      "infrastructure-as-code",
      "GitLab",
      "Terraform"
    ],
    "responsibilities": [
      "Build, deploy safely and incrementally and operate critical production systems with focus on scalability, reliability, observability, performance and security.",
      "Monitor, support and enhance product deployment experience across services.",
      "Build automation to remove toil and efficiently operate production systems.",
      "Proactively monitor, respond to, and enhance alerts and set up automated alert handling.",
      "Create and maintain the incident response runbooks.",
      "Build and deploy new systems with scalability, reliability, and observability as primary requirements.",
      "Triage platform/infrastructural issues and help Arista software engineers in their triages. Engage with 3rd party vendor support.",
      "Deploy new systems in a staged manner.",
      "Write postmortem documents and build solutions to avoid incidents from repeating.",
      "Plan and communicate maintenance windows on production systems.",
      "Work with Arista’s product development teams to identify infrastructural issues that are causing bottlenecks and limitations in their workflows. Design and implement solutions to resolve them.",
      "Survey and adopt best practices around infrastructure/platform to maintain secure, scalable and fault-tolerant systems.",
      "Implement solutions to scale the systems.",
      "Implement fault-tolerance and performance to improve availability of the systems.",
      "Study the design and sufficient implementation details of OSS systems for better triage and fix resolution."
    ],
    "requirements": [
      "Bachelors in Computer Science or Engineering + 5 years’ experience, MS Computer Science or Engineering + 5 years’ experience, or equivalent work experience.",
      "Knowledge of one or more of Go, Python, bash shell scripting to be able to implement medium complexity automation workflows.",
      "Knowledge of Linux (or UNIX) from administration and debugging perspective.",
      "Hands-on experience in operating software systems (infrastructure, complex applications etc) at scale.",
      "Experience in server provisioning (esp from storage and networking perspective).",
      "Strong problem solving and software troubleshooting skills.",
      "Experience with infrastructure-as-code."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Arista Networks is an industry leader in data-driven, client-to-cloud networking for large data center, campus and routing environments. As an SRE at Arista, you’ll be part of the team responsible for our global service fleet, ensuring the reliability and performance of our CloudVision service fleet.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4353767191&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "SPOTIO",
    "position": "Senior Site Reliability Engineer (DevOps)",
    "location": "Gdansk",
    "salary": "",
    "tech_stack": [
      "Azure",
      "Kubernetes",
      "Cloudflare",
      "Elasticsearch",
      "Pulumi",
      "GitHub"
    ],
    "nice_to_have_stack": [
      ".NET",
      "JavaScript",
      "TypeScript",
      "Prometheus",
      "Grafana",
      "Datadog",
      "ELK"
    ],
    "responsibilities": [
      "Ensure the availability, performance, and scalability of SPOTIO's production systems and infrastructure.",
      "Define, implement and monitor SLIs/SLOs, error budgets, capacity planning, incident response and root-cause workflows.",
      "Build and evolve automation tooling: provisioning, deployments (CI/CD), monitoring, alerting, self-healing mechanisms, infrastructure as code.",
      "Partner with software engineering teams to help design Highly Available and scalable systems.",
      "Manage and optimize our cloud stack on Azure: compute, networking, storage, identity, security, cost-optimization, disaster recovery, high-availability.",
      "Work with tools such as Azure, Cloudflare, Elasticsearch, Pulumi, Kubernetes, and GitHub.",
      "Build and maintain robust CI/CD pipelines (automation of builds, tests, releases, rollbacks) to accelerate safe feature delivery.",
      "Participate in on-call rotations, perform incident triage, drive post-mortem analyses and remediation.",
      "Advocate for reliability culture: mentor engineers, evangelize best practices, create documentation, run disaster recovery exercises."
    ],
    "requirements": [
      "5+ years of experience in a Site Reliability, Platform, or DevOps role at scale (cloud-native environment).",
      "3+ years of experience with containerization and orchestration (Kubernetes, AKS).",
      "3+ years of experience with Microsoft Azure (IaaS, PaaS, networking, security, monitoring).",
      "Hands-on experience with Cloudflare (CDN, DNS, WAF or similar), Elasticsearch (deployment/management/observability) and Pulumi (or similar IaC: Terraform, CDK).",
      "Strong automation mindset and tooling experience: build/operate CI/CD pipelines, scripting (Python, .NET, JavaScript, or similar).",
      "Experience with version control (Git / GitHub), branching strategies, release management.",
      "Excellent troubleshooting skills. E.G. Strong ability to dig into production issues, latency, MTTR, and drive remediation.",
      "Strong communication and collaboration skills w/ experience working across teams.",
      "Proactive, self-driven, and comfortable in a rapidly-evolving startup/scale-up environment."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "SPOTIO is a dynamic, fast-growing American start-up with a 10-year tradition of creating the #1 Sales Engagement Platform, helping field sales teams manage sales activities, increase the productivity of sales representatives, and record field sales insights.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4338367037&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Inter Cars S.A.",
    "position": "Platform Kubernetes Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Kubernetes",
      "Rancher",
      "VMware",
      "Go",
      "Java",
      "Python",
      "Terraform",
      "Ansible",
      "Helm",
      "ArgoCD",
      "Kustomize",
      "Prometheus",
      "Grafana",
      "Loki"
    ],
    "nice_to_have_stack": [
      "ELK",
      "Linux",
      "Bash",
      "Azure"
    ],
    "responsibilities": [
      "Development and administration of Kubernetes clusters using Rancher (VM on VMware and bare-metal).",
      "Configuration, monitoring, and scaling of Kubernetes infrastructure.",
      "Managing security policies, RBAC, and network in the cluster.",
      "Automation of deployment and operational processes (Helm, ArgoCD, Kustomize).",
      "Integration of Kubernetes with monitoring and logging systems (Prometheus, Grafana, Loki).",
      "Designing, developing, and maintaining an internal developer platform.",
      "Collaborating with development teams to improve Developer Experience."
    ],
    "requirements": [
      "Minimum 3 years of experience in software development (Go / Java / Python).",
      "Experience in administration and maintenance of Kubernetes environments.",
      "Ability to diagnose network, performance, and application problems.",
      "Experience in automation of infrastructure management (Terraform, Ansible, Terragrunt, Crossplane, Pulumi).",
      "Ability to create and maintain technical documentation.",
      "Collaboration with development and DevOps teams to optimize applications for Kubernetes."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Inter Cars S.A. is the largest distributor of automotive spare parts in Central and Eastern Europe. The IT Infrastructure Department maintains, develops, and monitors key components of IT infrastructure located in three data centers in Warsaw, serving all companies in the Inter Cars SA capital group operating in 18 countries. We are looking for a Platform Kubernetes Engineer to support the team in building and developing a new internal platform based on Kubernetes in an on-premises model.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4350781488&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Arcos LLC",
    "position": "CloudOps Engineer",
    "location": "Warsaw",
    "salary": "55000 - 70000 USD",
    "tech_stack": [
      "AWS",
      "EC2",
      "RDS",
      "S3",
      "DynamoDB",
      "Elasticache",
      "Route53",
      "PostgreSQL",
      "Oracle",
      "SQL Server",
      "Ansible",
      "Terraform",
      "Apache",
      "Nginx",
      "Tomcat",
      "NodeJS",
      "Bash",
      "Python",
      "PowerShell",
      "Kubernetes",
      "Docker",
      "Jira",
      "Confluence"
    ],
    "nice_to_have_stack": [
      "ESRI ArcGIS Server",
      "FME Data Integration tools"
    ],
    "responsibilities": [
      "Design, develop and maintain scalable AWS solutions and infrastructure.",
      "Develop tooling and processes to automate the deployment of SaaS based applications and their underlying operating systems and infrastructure.",
      "Perform PostgreSQL and Oracle database administration.",
      "Partner with Engineering, Development, Quality Assurance, Professional Services, and Technical Support to ensure the success of the assigned product offerings and schedules.",
      "Engage in Agile team practices.",
      "Participate in 24x7 on-call responsibilities, maintaining the availability and performance of all customer-facing production services."
    ],
    "requirements": [
      "Bachelor's degree in Computer Science or related field, or equivalent work experience.",
      "4-5 years of system administration experience, ideally in global management and operations of highly trafficked production applications.",
      "Experience working in a 24x7 SaaS environment is preferred.",
      "4-5 years of experience designing solutions for and managing AWS services, including but not limited to: EC2, RDS, S3, DynamoDB, Elasticache, WAF/Shield, Route53, IAM and Directory Service, ECS, EKS, ECR, DNS, Parameter Store, ALB.",
      "2 years of experience with CI/CD technologies and best practices using AWS CodePipeline, CodeBuild, Github Actions or Bitbucket Pipelines.",
      "2 years of experience with PostgreSQL, Oracle, SQL Server.",
      "Experience with Linux and Windows system administration, automation and performance tuning.",
      "Experience with scripting languages, including Bash, Python and Powershell.",
      "Advanced knowledge of system vulnerability management and security best practices."
    ],
    "work_mode": "remote",
    "employment_type": "contract",
    "seniority": null,
    "description": "Arcos provides SaaS solutions to solve resource management challenges that companies face. The Arcos Resource Management platform helps customers plan, respond, restore, and report actions taken during normal operations or unplanned service interruptions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4365067204&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Set2Recruit",
    "position": "DevOps Engineer",
    "location": "Remote",
    "salary": "100000 - 110000 EUR",
    "tech_stack": [
      "automation",
      "containerised systems",
      "Linux",
      "networking",
      "secure communication"
    ],
    "nice_to_have_stack": [
      "cryptographic primitives",
      "trust models",
      "API"
    ],
    "responsibilities": [
      "Designing how research-grade technology becomes something stable, repeatable, and supportable.",
      "Owning the mechanics of change for systems where correctness matters more than speed.",
      "Building automation that enforces safety, consistency, and reversibility by default.",
      "Running distributed platforms and the trust, networking, and lifecycle layers beneath them.",
      "Making systems explain themselves through health signals, behaviour, and failure modes.",
      "Improving reliability practices, recovery paths, and institutional memory when things break.",
      "Acting as a technical counterpart to engineers and researchers, shaping designs with real-world constraints in mind."
    ],
    "requirements": [
      "Proven experience owning production platforms end-to-end.",
      "Strong automation instincts and comfort expressing infrastructure and workflows as code.",
      "Deep hands-on experience with containerised systems and their operational edges.",
      "Solid grounding in Linux, networking, and secure communication patterns.",
      "The ability to debug complex behaviour without relying on perfect documentation.",
      "A security-first mindset that assumes today’s “safe enough” may not be tomorrow’s.",
      "Clear written thinking — especially when defining how systems should be operated long after you’ve moved on."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "We’re a research-driven team working helping our cutting edge technology client that is building new tech that designed to stay relevant as today’s security assumptions start to age out.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4353934493&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Ideamotive",
    "position": "DevOps/Observability Engineer",
    "location": "Warsaw",
    "salary": "18000 - 24800 PLN",
    "tech_stack": [
      "ELK",
      "Loki",
      "Terraform",
      "Ansible",
      "Docker",
      "Kubernetes",
      "Jenkins",
      "GitLab CI",
      "Prometheus",
      "Grafana",
      "Bash",
      "Python"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "budować i rozwijać systemy monitoringu oraz observability dla aplikacji i infrastruktury",
      "zarządzać logowaniem i agregacją logów (ELK / Loki)",
      "wdrażać i rozwijać distributed tracing",
      "współtworzyć architekturę observability (również w środowiskach chmurowych i Kubernetes)",
      "wspierać zespoły developerskie w analizie incydentów i poprawie widoczności aplikacji",
      "zarządzać infrastrukturą jako kodem (Terraform, Ansible)",
      "pracować z Dockerem i Kubernetesem",
      "projektować i utrzymywać pipeline’y CI/CD (Jenkins, GitLab CI)",
      "monitorować aplikacje i infrastrukturę przy użyciu Prometheus, Grafana, ELK",
      "integrować bezpieczeństwo w pipeline’ach CI/CD (DevSecOps)",
      "automatyzować procesy (Bash, Python)",
      "uczestniczyć w pracach R&D oraz testowaniu nowych narzędzi i rozwiązań DevOps"
    ],
    "requirements": [
      "praktycznego doświadczenia z narzędziami do monitoringu, metryk i alertowania",
      "znajomości logowania w środowiskach rozproszonych (np. Loki, Elasticsearch)",
      "doświadczenia z tracingiem i narzędziami observability",
      "umiejętności automatyzacji (Bash, Python, Ansible)",
      "wiedzy o sposobach zbierania metryk (scraping, service discovery, push gateway)",
      "znajomości dobrych praktyk SRE (SLO, SLA, RCA, error budget)",
      "doświadczenia z CI/CD, GitOps i integracją observability",
      "znajomości systemów kontroli wersji",
      "wiedzy z zakresu bezpieczeństwa w DevOps",
      "doświadczenia z chmurą publiczną",
      "umiejętności dokumentowania i dzielenia się wiedzą",
      "dobrej komunikacji i chęci współpracy z developerami, testerami i zespołami bezpieczeństwa",
      "znajomości języka polskiego na poziomie B2/C1"
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Do zespołu realizującego projekty w sektorze bankowości szukamy osoby, która chce rozwijać obszar DevOps i Observability w dużym, złożonym i dojrzałym środowisku technologicznym.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4360772121&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Extreme Networks",
    "position": "Staff Cloud Operations Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Kubernetes",
      "AWS",
      "GCP",
      "Azure",
      "Terraform",
      "GitOps",
      "ArgoCD",
      "Helm",
      "Prometheus",
      "Grafana",
      "Loki",
      "Elasticsearch",
      "PostgreSQL",
      "Redis",
      "KAFKA",
      "RabbitMQ"
    ],
    "nice_to_have_stack": [
      "multi-cloud architectures",
      "Istio",
      "Linkerd"
    ],
    "responsibilities": [
      "Design and implement multi-cluster, multi-region Kubernetes deployments using EKS, GKE, and AKS.",
      "Take end-to-end ownership of production infrastructure.",
      "Build and maintain Terraform modules for complex infrastructure patterns.",
      "Design and optimize ArgoCD ApplicationSets and Helm chart architectures.",
      "Analyze system performance, identify bottlenecks, and implement optimizations.",
      "Build and enhance monitoring, alerting, and observability using Prometheus, Grafana, Loki, and custom tooling.",
      "Implement security controls, compliance frameworks, and best practices across cloud infrastructure.",
      "Mentor engineers, establish best practices, and drive technical decisions."
    ],
    "requirements": [
      "5+ years in cloud infrastructure engineering, with deep expertise in at least one major cloud provider (AWS preferred)",
      "Strong Kubernetes experience: cluster design, operators, controllers, and multi-cluster management",
      "Proficiency with Infrastructure as Code: Terraform, CloudFormation, or similar",
      "GitOps expertise: ArgoCD, Flux, or similar; experience with ApplicationSets and complex deployment patterns",
      "Deep Linux and networking knowledge",
      "Experience with distributed systems: Elasticsearch, PostgreSQL, Redis, Kafka, RabbitMQ",
      "Monitoring and observability: Prometheus, Grafana, ELK stack, or similar",
      "Strong problem-solving skills and experience debugging complex distributed systems",
      "Experience with cloud security, compliance (SOC2, ISO27001), and secure-by-design practices",
      "Excellent communication skills for working across time zones and with distributed teams",
      "Self-directed with a track record of owning problems end-to-end"
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Extreme’s Cloud Operations team is a group of talented engineers passionate about building highly reliable, scalable and secure solutions in public/private cloud environments. We are looking to hire a highly motivated Cloud Operations engineer with strong working experience in production operation and deployment automation. You will work with the team to design, develop and implement deployment automation solutions end-to-end.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4321052980&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Vertex Agility",
    "position": "DevSecOps Engineer",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "Python",
      "Jenkins",
      "Groovy",
      "Java",
      "Maven",
      "npm",
      "Terraform",
      "Helm",
      "SonarQube",
      "Sonatype IQ"
    ],
    "nice_to_have_stack": [
      "GCP",
      "AWS",
      "GitOps"
    ],
    "responsibilities": [
      "Build and improve secure, scalable CI/CD pipelines and delivery workflows.",
      "Automate tooling and processes using Python, Jenkins, and Groovy libraries.",
      "Strengthen software supply-chain security with SBOMs, digests, and integrity checks.",
      "Integrate and manage code quality, SCA, SAST, and container scanning tools.",
      "Optimize pipeline speed and reliability through caching, parallelization, and dependency cleanup.",
      "Collaborate with engineering teams to enforce secure-by-default and compliant delivery practices."
    ],
    "requirements": [
      "Solid engineering experience, with focused on CI/CD platform work or DevSecOps roles.",
      "Strong mastery of Jenkins, including building and maintaining Groovy shared-library pipelines.",
      "Advanced automation skills using Python (tooling, JSON/YAML processing, custom scripts).",
      "Deep experience with packaging and build tooling across ecosystems (Java/Maven, npm, Python packaging).",
      "Familiarity with container workflows, Helm, Terraform, and container image metadata management.",
      "Proven supply-chain security awareness: SBOM generation / artifact integrity / digest-based validation / dependency hygiene.",
      "Working knowledge of security and scanning tools: SAST, container scanning, SCA, code-quality tools (e.g. SonarQube, Sonatype IQ).",
      "Track record of optimizing CI/CD performance (caching, parallel builds, dependency cleanup).",
      "Strong sense of compliance, governance, and secure coding practices."
    ],
    "work_mode": "hybrid",
    "employment_type": "contract",
    "seniority": null,
    "description": "Vertex Agility is your next Global Digital Consultancy, providing deep expertise in Cloud, Data, and Software to the biggest brands in the world and the most disruptive scale-ups.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4352648876&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "RTB House",
    "position": "DevSecOps Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "Google Cloud Platform",
      "Kubernetes",
      "Argo CD",
      "PostgreSQL",
      "GitHub",
      "Terraform",
      "GitHub Actions"
    ],
    "nice_to_have_stack": [
      "SIEM solutions",
      "zero trust architecture",
      "advanced threat detection tools"
    ],
    "responsibilities": [
      "Contributing to the design, implementation, and maintenance of our secure cloud infrastructure, CI/CD pipelines, and deployment automation.",
      "Integrating and implementing best-in-class security standards and practices throughout the software development lifecycle.",
      "Proactively identifying, addressing, and preventing vulnerabilities before they impact our systems.",
      "Partnering with cross-functional teams to establish and enforce DevSecOps standards."
    ],
    "requirements": [
      "3+ years of experience as a DevSecOps Engineer or in a closely related role.",
      "Significant experience with at least one cloud platform (GCP, AWS, or Azure), with a preference for GCP.",
      "Hands-on experience with CI/CD tools such as GitHub Actions.",
      "Expertise in infrastructure-as-code (IaC) tools like Terraform.",
      "Strong Python programming skills for building automation tools, security checks, and integration scripts.",
      "Strong knowledge of Docker for containerization.",
      "Demonstrated experience with shift-left security practices, including self-healing, automated infrastructure and continuous security testing (SAST, DAST, dependency scans).",
      "Familiarity with a range of security tools (e.g., vulnerability scanners, secrets management platforms, container security solutions).",
      "Knowledge of encryption methods, access control (IAM), and compliance frameworks.",
      "Experience in container image scanning, threat modelling, and incident response workflows.",
      "A strong commitment to DevSecOps best practices and high-security standards in designed solutions."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "RTB House is a next-generation performance demand-side platform (DSP) that uses proprietary Deep Learning AI algorithms to help brands grow. As a DevSecOps Engineer, you will influence our technology landscape by contributing to secure cloud infrastructure and ensuring security is integrated into every stage of product development.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4360901872&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "RSight",
    "position": "Cloud DevOps Engineer",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Python",
      "Docker"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Projektowanie, wdrażanie i zarządzanie infrastrukturą chmurową AWS zgodnie z najlepszymi praktykami dotyczącymi skalowalności, niezawodności i bezpieczeństwa.",
      "Tworzenie i utrzymywanie Infrastruktur jako Kod (IaC) z użyciem AWS CDK lub CloudFormation.",
      "Budowanie, utrzymywanie i optymalizacja środowisk chmurowych, w tym środowisk deweloperskich, QA, staging oraz produkcyjnych.",
      "Wdrażanie i zarządzanie frameworkami obserwowalności obejmującymi logowanie, monitorowanie, alertowanie i metryki.",
      "Automatyzacja procesów budowania, wdrażania i wydawania oprogramowania poprzez solidne pipeline’y CI/CD.",
      "Konteneryzacja aplikacji przy użyciu Dockera i integracja kontenerów z procesami wdrożeniowymi.",
      "Tworzenie skryptów automatyzacyjnych i narzędzi wewnętrznych w Pythonie.",
      "Rozwiązywanie problemów związanych z infrastrukturą chmurową, siecią i procesami wdrożeniowymi.",
      "Współpraca z zespołami deweloperskimi, architektami i QA w celu zapewnienia płynnych i efektywnych cykli dostaw."
    ],
    "requirements": [
      "Dogłębna wiedza z zakresu architektury chmury AWS i kluczowych usług, takich jak EC2, ECS/EKS, Lambda, S3, VPC, CloudWatch oraz IAM.",
      "Praktyczne doświadczenie z Infrastrukturą jako Kod przy użyciu AWS CDK lub CloudFormation.",
      "Biegłość w Pythonie w kontekście automatyzacji i tworzenia narzędzi.",
      "Doświadczenie z Git oraz nowoczesnymi narzędziami CI/CD, takimi jak GitHub Actions, GitLab CI lub Jenkins.",
      "Bardzo dobra znajomość Dockera i wdrażania aplikacji w kontenerach.",
      "Znajomość narzędzi do obserwowalności, takich jak CloudWatch, Prometheus, Grafana lub ELK/OpenSearch.",
      "Solidne zrozumienie automatyzacji wydań, strategii wdrożeń i zarządzania środowiskami.",
      "Dobra znajomość podstaw sieci: VPC, podsieci, routing, grupy bezpieczeństwa.",
      "Doświadczenie w pracy w zwinnych, dynamicznych środowiskach inżynieryjnych.",
      "Doskonałe umiejętności komunikacyjne i współpracy."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": null,
    "description": "RSight® to nowoczesne rozwiązanie rekrutacyjne oparte na wartościach sprawiedliwości, transparentności i podejściu skoncentrowanym na człowieku. Celem RSight® jest redefinicja procesu rekrutacji — stawiając ludzi i ich potencjał w centrum każdej szansy zawodowej.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4342753830&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Remobi",
    "position": "Senior DevOps Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Kubernetes",
      "Docker",
      "Terraform",
      "Prometheus",
      "Grafana"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Design, build, and maintain AWS-based cloud infrastructure for scalable AI workloads.",
      "Own CI/CD pipelines, deployment automation, and infrastructure as code using Terraform.",
      "Operate and scale containerised environments using Docker and Kubernetes.",
      "Implement and improve monitoring, alerting, and observability using Prometheus and Grafana.",
      "Strengthen platform security, reliability, and operational best practices.",
      "Work closely with backend, platform, and AI engineers to support rapid product delivery.",
      "Take responsibility for uptime, performance, and continuous improvement in production environments."
    ],
    "requirements": [
      "Strong hands-on expertise with AWS, Kubernetes, Docker, and Terraform.",
      "Proven experience owning CI/CD pipelines in high-performance environments.",
      "Solid understanding of security, monitoring, observability, and infrastructure reliability.",
      "Confidence operating production systems and taking ownership end-to-end."
    ],
    "work_mode": "remote",
    "employment_type": "contract",
    "seniority": "Seniority.senior",
    "description": "We are recruiting on behalf of a high-growth AI company operating at the intersection of AI, FinTech, and cloud infrastructure, building production-grade platforms where reliability, security, and observability are mission-critical.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4354044354&geoId=105072130&keywords=sre&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Comarch",
    "position": "Inżynier systemowy/ DevOps (K/M/X)",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "Windows Server",
      "Linux",
      "MS SQL",
      "Jenkins",
      "Docker"
    ],
    "nice_to_have_stack": [
      "Kubernetes"
    ],
    "responsibilities": [
      "Samodzielne wykonywanie i usprawnianie prac technicznych, serwisowych, instalacyjnych oraz konfiguracyjnych związanych z wdrażaniem Comarch ECM",
      "Instalacja, konfiguracja i patchowanie środowisk developerskich, testowych oraz produkcyjnych",
      "Zarządzanie i administracja środowiskami developerskimi, testowymi i produkcyjnymi, zapewniając ciągłość ich działania",
      "Administrowanie serwerami systemów operacyjnych: Windows oraz Linux",
      "Administrowanie bazami danych MS SQL Server",
      "Administrowanie środowiskami Jenkins i Docker, wspierając procesy CI/CD"
    ],
    "requirements": [
      "Wykształcenie informatyczne lub pokrewne",
      "Minimum rok doświadczenia w pracy w administracji systemami IT lub w obszarze DevOps",
      "Dobra znajomość Windows Server lub Linux",
      "Znajomość zagadnień związanych z architekturą serwerową we wdrożeniach (np. sieci, bezpieczeństwo)",
      "Mile widziane doświadczenie w pracy z narzędziami takimi jak Jenkins, Docker lub Kubernetes",
      "Umiejętność pracy z dokumentacją techniczną i rozwiązywania problemów",
      "Dobra znajomość języka angielskiego (poziom min. B2)",
      "Samodzielność w działaniu, zaangażowanie, odpowiedzialność, chęć do rozwiązywania problemów",
      "Umiejętność współpracy w zespole"
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "Comarch is implementing and developing Comarch ECM (Enterprise Content Management) solution, allowing clients to manage documents rapidly and securely, automate processes (workflow), integrate with systems and Active Directory, and legally compliant archiving.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4342218893&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Astek",
    "position": "DevOps Inżynier / Inżynierka",
    "location": "Warsaw",
    "salary": "1000 - 1150 PLN",
    "tech_stack": [
      "Jenkins",
      "Groovy",
      "GitHub",
      "Maven",
      "Sonar",
      "Nexus",
      "Docker",
      "Kubernetes",
      "Oracle",
      "PostgreSQL",
      "Grafana",
      "Kibana",
      "Elasticsearch"
    ],
    "nice_to_have_stack": [
      "Java",
      "Spring",
      "Azure"
    ],
    "responsibilities": [
      "Wdrażanie aplikacji i utrzymanie komponentów infrastruktury w prywatnej chmurze.",
      "Rozwój i optymalizacja pipeline’ów CI/CD.",
      "Implementacja mechanizmów bezpieczeństwa (certyfikaty, szyfrowanie).",
      "Monitorowanie infrastruktury i aplikacji (Grafana, Kibana, Elasticsearch).",
      "Zapewnienie wsparcia produkcyjnego (incident i change management).",
      "Współpraca w Agile i tworzenie dokumentacji technicznej."
    ],
    "requirements": [
      "Posiadasz min. 5 lat doświadczenia w DevOps / Administracji.",
      "Posiadasz doświadczenie w utrzymaniu systemów Linux/Windows.",
      "Posiadasz doświadczenie w zakresie tworzenia pipeline'ów CI/CD oraz znasz narzędzia takie jak: Jenkins, Groovy, GitHub, Maven, Sonar, Nexus.",
      "Posiadasz doświadczenie w konteneryzacji i orkiestracji (Docker, Kubernetes) oraz hostingu maszyn wirtualnych i zarządzanych bazach danych (Oracle, PostgreSQL).",
      "Znasz narzędzia do monitoringu: Grafana, Kibana, Elasticsearch.",
      "Posiadasz doświadczenie w pracy Agile.",
      "Bardzo dobrze znasz język angielski.",
      "Jesteś otwarty/a na sporadyczną pracę zmianową (9:00–19:00).",
      "Cechują Cię doskonałe umiejętności analityczne oraz nastawienie na ciągłe doskonalenie."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "Currently, for our client from the financial industry, we are looking for a candidate for the position of DevOps Engineer. This role involves implementing and maintaining infrastructure components in a private cloud, optimizing CI/CD pipelines, and ensuring security mechanisms.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4353003511&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "ICEO - Venture Builder",
    "position": "Senior DevOps/ SRE",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Python",
      "GoLang",
      "C++",
      "Java",
      "Linux",
      "Debian",
      "Ubuntu",
      "HTTP(s)",
      "DNS",
      "SSH",
      "TCP/IP",
      "REST",
      "Prometheus",
      "Grafana",
      "KAFKA",
      "Redis",
      "Nginx",
      "Docker",
      "Kubernetes",
      "Helm",
      "GCP"
    ],
    "nice_to_have_stack": [
      "Argo CD",
      "Apache HTTP Server",
      "OpenVPN",
      "Kibana",
      "FluentD",
      "Elasticsearch",
      "Grafana Loki",
      "PostgreSQL",
      "Okta"
    ],
    "responsibilities": [
      "You will have a direct impact on the product, working closely across DevOps, SRE, and infrastructure.",
      "As a key driver of infrastructure, you will be collaborating in multi-disciplinary teams (4 people) focused on developing business functionality from an infrastructure perspective.",
      "Daily syncs with the DevOps team and participation in lodge meetings will provide opportunities to share knowledge and solve challenges alongside backend developers and fellow DevOps engineers.",
      "Own and lead the definition and execution of the SRE vision and strategy, ensuring alignment with business objectives and engineering priorities.",
      "Architect, maintain and develop infrastructure within GCP and GKE - on high and low-level design for performance at all levels and with security, availability and reliability at the core of it.",
      "Develop automated solutions for system reliability, capacity planning, and incident response to minimise manual intervention.",
      "Cooperate with engineering and product teams to design and implement highly available and fault-tolerant systems.",
      "Own and deliver Service Level Objectives, Service Level Indicators and error budgets to enhance system reliability.",
      "Create documentation from the implemented solutions.",
      "Influence and mentor engineering teams on SRE principles, DevOps culture, and best practices.",
      "Keep up with industry trends, leveraging new tools, frameworks and methodologies to consistently enhance system reliability.",
      "Care for keeping the right balance between a high level of security and comfort, and flexibility of teamwork.",
      "Participate in daily and planning meetings."
    ],
    "requirements": [
      "At least 5+ years of experience as a DevOps, SRE, or in a similar role, working on a product and long-term platform maintenance.",
      "At least 10+ years of experience in technology.",
      "Experience in independently managing a platform and the ability to make autonomous decisions whilst risk assessment.",
      "Knowledge of at least one programming language: Python, GoLang, C++ or Java.",
      "Wide experience with JVM & Node.js technologies in terms of maintaining the applications.",
      "Practical knowledge and experience in using Linux at an administrative level (Debian/Ubuntu).",
      "Knowledge of issues concerning LAN/WAN networks, Firewall, proxy servers, Load Balancers, and popular networking protocols: HTTP(s), DNS, SSH, TCP/IP, and network services REST.",
      "Practical expertise in observability tools (e.g., Prometheus, Grafana, Grafana Mimir, OpenTelemetry).",
      "Knowledge of technologies: Kafka, Redis, Nginx.",
      "Knowledge of the containerization system Docker.",
      "Knowledge of CI/CD systems and version control.",
      "Knowledge of the Kubernetes platform and Helm.",
      "Experience in using at least one public cloud operator: GCP, AWS, Azure.",
      "Experience implementing redundancy and disaster recovery scenarios.",
      "Practical expertise in designing, implementing, and maintaining scalable, high-performance infrastructure with hands-on experience in production-grade systems, using tools & technologies like HPA, KEDA, Affinity & Anti-affinity rules.",
      "Proficient in both written and spoken English, at a B2 level or higher."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "BeOne (stealth mode project) is a next-generation neobank that redefines how individuals and businesses manage money by blending traditional and digital finance. Our platform offers multi-currency accounts, ultra-low fees, real-time global payments, and robust financial tools, all within an intuitive, refined interface.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4360940454&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "ASBIS",
    "position": "Observability DevOps",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Java",
      "DB2",
      "Python",
      "PostgreSQL",
      "Nuxt.js",
      "PHP",
      "Zabbix",
      "Grafana",
      "Elastic Stack",
      "Prometheus",
      "Alertmanager",
      "Loki",
      "OpenTelemetry",
      "Ansible",
      "Terraform",
      "Puppet",
      "GitLab CI/CD",
      "Jenkins",
      "Cloudflare",
      "DigitalOcean"
    ],
    "nice_to_have_stack": [
      "null"
    ],
    "responsibilities": [
      "Design, deploy, and maintain observability platforms including Zabbix, Grafana, and Elastic Stack (Elasticsearch, Logstash, Kibana).",
      "Implement and maintain metrics, logs, traces, and synthetic monitoring across infrastructure and applications.",
      "Integrate Prometheus, Alertmanager, Loki, and OpenTelemetry where applicable for unified observability.",
      "Develop dashboards and alerting logic to ensure timely and actionable incident notifications.",
      "Maintain monitoring coverage for Linux, Windows, network devices, applications, and cloud services.",
      "Automate deployment and configuration of monitoring components using Ansible, Terraform, or Puppet.",
      "Manage configuration templates and Zabbix host provisioning through CI/CD pipelines.",
      "Leverage APIs and scripting (Python, PowerShell, Bash) for data collection and automation.",
      "Collaborate with SRE, DevOps, and IT Operations teams to improve system reliability and MTTR (Mean Time to Recovery).",
      "Maintain accurate system documentation, monitoring standards, and runbooks.",
      "Define KPIs, SLIs, SLOs, and SLA measurement processes in coordination with service owners."
    ],
    "requirements": [
      "4+ years of experience in DevOps or SRE roles.",
      "Strong background in CI/CD automation and infrastructure as code.",
      "Knowledge of Hardware/OS/Applications level metrics defaults and normals.",
      "Understanding of monitoring and observability systems.",
      "Experience with both on-premises and cloud-based environments.",
      "Solid communication skills and a proactive, problem-solving mindset."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "ASBIS is expanding its internal IT department and looking for an Observability DevOps Engineer to design and build a transparent, scalable, and reliable infrastructure for three key internal projects: Global ERP, Local ERP, and Websites.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4354275636&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Brembo",
    "position": "IT Infrastructure Specialist",
    "location": "Dąbrowa Górnicza",
    "salary": "",
    "tech_stack": [],
    "nice_to_have_stack": [],
    "responsibilities": [
      "To ensure the installation and maintenance of the enterprise system of IT Infrastructure.",
      "To ensure integration management and maintenance of equipment and applications, providing ongoing technical support.",
      "To support the development of an infrastructure plan that continues to meet the future needs of the organization.",
      "To ensure service levels to the business, in compliance with Brembo's guidelines."
    ],
    "requirements": [],
    "work_mode": "onsite",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Brembo leads the world in the design and production of high-performance braking systems and components for top-flight manufacturers of cars, motorbikes and commercial vehicles. Founded in 1961 in Italy, the company has a long-standing reputation for providing innovative solutions for original equipment and aftermarket. Brembo is a key player in the world of racing: for 50 years, it has been the choice of the Teams competing in the most demanding motorsport championships worldwide and it has won more than 700 titles.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=3778523493&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Software Mind",
    "position": "Cloud DevOps (Azure)",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "Microsoft Azure",
      "PowerShell",
      "CI/CD",
      "Git",
      "Bicep",
      "Terraform"
    ],
    "nice_to_have_stack": [
      "Azure cloud certifications",
      "Kubernetes"
    ],
    "responsibilities": [
      "Lead and coordinate incident response activities, including triage, impact analysis, and collaboration with engineering teams to ensure timely resolution.",
      "Prepare clear, detailed RCA reports following service restoration and ensure follow-up actions are completed.",
      "Enhance testing approaches, streamline operational processes, design reporting mechanisms, and automate repetitive tasks using scripting (PowerShell) and CI/CD practices.",
      "Create, update, and maintain runbooks, SOPs, architecture notes, and knowledge base articles to support operational consistency.",
      "Provision, configure, and maintain Azure resources across multiple environments using IaC tools such as Bicep.",
      "Implement and maintain monitoring, logging, and alerting solutions; ensure Azure infrastructure meets performance, availability, and reliability requirements.",
      "Support deployments, patching, DR procedures, and continuous improvement of the Azure platform."
    ],
    "requirements": [
      "5+ years in cloud operations, infrastructure delivery, SRE, or related roles with demonstrated process improvement achievements.",
      "Extensive hands-on knowledge of Microsoft Azure, including compute, storage, networking, identity, governance, and security services.",
      "Strong PowerShell scripting skills for automation and operational tooling.",
      "Experience with Azure DevOps, including designing and maintaining CI/CD pipelines.",
      "Proficiency with Git and modern version control practices.",
      "Hands-on experience with Bicep, Terraform (or other IaC tools).",
      "Solid understanding of networking concepts and cloud-native architectures.",
      "Strong communication capabilities, problem-solving mindset, and the ability to work effectively in a collaborative, fast-paced environment."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Software Mind develops solutions that make an impact for companies around the globe, focusing on operational excellence and cloud support maturity.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4344314608&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Tenth Revolution Group",
    "position": "Senior DevOps (AWS)",
    "location": "Warsaw",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Lambda",
      "ECS",
      "EKS",
      "API Gateway",
      "S3",
      "CloudFormation",
      "CDK",
      "CloudWatch",
      "Datadog"
    ],
    "nice_to_have_stack": [
      "Terraform"
    ],
    "responsibilities": [
      "Architect and develop AWS cloud-native applications using Lambda, ECS/EKS, API Gateway, S3, CloudFormation/CDK, and other services.",
      "Design and implement observability, monitoring, and alerting solutions (CloudWatch, Datadog, etc.).",
      "Lead system analysis, design documentation, and ensure solutions meet reliability, security, and scalability standards.",
      "Develop, test, and deploy high-quality application code, and troubleshoot complex distributed systems.",
      "Optimize cloud resources and CI/CD pipelines; improve IaC templates and automation.",
      "Guide teams on cloud architecture, DevOps best practices, resiliency, and observability.",
      "Conduct code reviews and provide technical mentorship."
    ],
    "requirements": [
      "5+ years software engineering experience, with 3+ years hands-on AWS development.",
      "Expertise in AWS cloud services, IaC (CloudFormation, CDK, Terraform), and distributed systems.",
      "Strong background in observability systems: metrics, logging, tracing, dashboards, alerts.",
      "Deep understanding of data structures, algorithms, and large-scale system design.",
      "Experience troubleshooting cloud issues, optimizing performance, and handling production incidents.",
      "Familiarity with containers, API design, networking, and cloud security best practices."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "We are looking for a Senior DevOps / Cloud Engineer to design, build, and maintain AWS cloud-native applications with a focus on scalability, resiliency, and operational excellence.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4355317371&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Roche",
    "position": "DevOps Infrastructure Engineering Expert",
    "location": "Poznan",
    "salary": "23000 - 28000 PLN",
    "tech_stack": [
      "Terraform",
      "CloudFormation",
      "Python",
      "Docker",
      "Kubernetes"
    ],
    "nice_to_have_stack": [
      "AWS"
    ],
    "responsibilities": [
      "This is a software development position to write high quality software that will perform at scale, be supportable, and be extensible.",
      "Ensure the software stack integrates with our existing CI/CD pipelines and contribute to continuous improvement; independently design, plan and deliver high-quality software.",
      "Providing technical support for day-to-day operations, tool integration, automation support, change management and business continuity programs.",
      "Lead, mentor, and coordinate a team of DevOps engineers to ensure the successful development, deployment, and maintenance of scalable, reliable, and efficient infrastructure solutions."
    ],
    "requirements": [
      "Experienced with the software development lifecycle and cross-functional teams, with skills in Python design patterns and Object-Oriented programming.",
      "Proficient in DevOps methodologies including CI/CD, containerization (Docker, Kubernetes), orchestration, and software development process automation."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.mid",
    "description": "Roche’s expanding digital product portfolio demonstrates its commitment to provide healthcare professionals, laboratories, and patients with digital and digitally-enabled solutions that transform patient care. The existing product portfolio includes decision support systems, data management solutions, and workflow solutions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4278050490&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Luxoft Poland",
    "position": "Senior DevOps (OpenSearch/ElasticSearch expert)",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "Elasticsearch",
      "OpenSearch",
      "ELK stack",
      "Python",
      "Bash",
      "PowerShell",
      "Docker",
      "Kubernetes"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Setup and ownership over the ElasticSearch and OpenSearch environments.",
      "Designing, implementing and improving build and release pipelines for cloud-based Analytics solutions.",
      "Helping the teams with CI/CD setup for automated testing, infrastructure provisioning, production releases.",
      "Identifying and addressing technical debt.",
      "Making sure cloud environments adhere to standards and policies, are kept clean and efficient.",
      "Troubleshooting and fixing issues related to cloud environments.",
      "Code delivery & review in Python.",
      "Identifying, implementing and improving automation in various areas of the cloud-based solutions."
    ],
    "requirements": [
      "Expertise in ElasticSearch architecture, operations, and the ELK stack (ElasticSearch, Logstash, Kibana).",
      "Knowledge of OpenSearch architecture, features, deployment, and configuration.",
      "SIEM (Security Information and Event Management) expertise, including security analytics and threat detection.",
      "Proficiency in designing and executing performance tests and analyzing performance data.",
      "Experience with RESTful APIs, API testing, and documentation.",
      "Scripting and automation skills (Python, Bash, PowerShell), and familiarity with CI/CD and DevOps practices.",
      "Understanding of authentication and authorization mechanisms, and experience with security compliance requirements.",
      "Skills in planning and executing large-scale data migrations and ETL processes.",
      "Knowledge of on-premise and cloud infrastructure, containerization, and orchestration (Docker, Kubernetes).",
      "Project management skills, including task planning, coordination, risk assessment, and mitigation.",
      "Technical writing skills for creating clear documentation and detailed reports.",
      "Experience with data visualization tools (Kibana, Grafana) and creating meaningful dashboards and reports."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "We're building a project for market leading large Germany based company with 440,000 customers worldwide. Originally known for leadership in enterprise resource planning (ERP) software, the company has evolved to become a market leader in end-to-end enterprise application software, database, analytics, intelligent technologies, and experience management. A top cloud company with 200 million users worldwide, the company helps businesses of all sizes and in all industries to operate profitably, adapt continuously, and achieve their purpose.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4362489806&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "ALTEN Polska",
    "position": "DevOps (IAM, Terraform & GCP)",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "IAM",
      "Terraform",
      "GCP",
      "CI/CD",
      "Kubernetes",
      "Docker",
      "KAFKA",
      "Neo4j"
    ],
    "nice_to_have_stack": [
      "Ansible",
      "Helm",
      "Kustomize",
      "GKE"
    ],
    "responsibilities": [
      "Design, build, and maintain CI/CD pipelines for IAM components, policies, connectors, microservices, and integrations.",
      "Enable automated testing, security scanning, and controlled deployments across DEV/TEST/PROD environments.",
      "Implement continuous improvement to streamline IAM release processes.",
      "Develop and maintain IaC (Terraform, Ansible for deploying IAM infrastructure, identity policies, directories, and supporting platforms.",
      "Ensure consistent, repeatable environments and compliance with architectural standards.",
      "Develop scripts and automation for account lifecycle operations, access provisioning, and system integrations.",
      "Deploy IAM services or related microservices on Kubernetes, cloud-native platforms, and serverless environments.",
      "Manage containerisation, service mesh integrations, certificates, and secrets for IAM workloads.",
      "Embed security into the build and deployment process, including vulnerability scanning, secrets detection, and code quality checks.",
      "Work with cybersecurity teams to ensure compliance with Zero Trust principles and IAM security policies.",
      "Collaborate with IAM architects, security teams, and application owners on design and integration requirements.",
      "Create and maintain documentation for pipelines, IaC, deployment patterns, and operational processes."
    ],
    "requirements": [
      "Infrastructure as code (Terraform & GCP Provisioning) Terraform core, GCP Infra, Policy as code, develop the capability to manage, maintain and write policies.",
      "Containerization & Kubernetes (GKE), Docker, Kubernetes, Helm / Kustomize, GKE Ops.",
      "CI/CD engineering, pipeline authoring, artifacts management, testing automation, deployment strategy.",
      "Data pipeline and DevOps (KAFKA / PubSub) – Kafka basics, schema registry, streaming infra and monitoring.",
      "Graph platform engineering, Neo4j basics, backups, recovery and DR GDS/APOC, observability.",
      "DevSecOps & platform security, security scanning, IAM and identity, network security, compliance.",
      "Release engineering and governance, release ops, change management, documentation.",
      "Regionalisation and compliance, regional deployments and failover, data residency, conditional access."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "ALTEN Polska supports its customers’ development strategies in the areas of innovation, R&D, and technological information systems. The DevOps Engineer will play a key role in delivering and operating enterprise Identity & Access Management (IAM) capabilities.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4364069971&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Intellias",
    "position": "SAP CI Engineer",
    "location": "Remote",
    "salary": "",
    "tech_stack": [
      "AWS",
      "Azure",
      "Terraform",
      "MuleSoft",
      "Solace",
      "SAP CI",
      "Biztalk",
      "Kubernetes",
      "Splunk",
      "DevOps"
    ],
    "nice_to_have_stack": [
      "KAFKA",
      "MQTT",
      "JMS"
    ],
    "responsibilities": [
      "Support implementation of the project’s Architecture Services and all related service portfolios at IT product level, including documentation standards, to better support current and future business needs.",
      "Develop and support integration models for different types of applications and endpoints, prioritizing API-First approach and re-usability with MuleSoft and EDA with Solace, SAP CI and Azure Integration Services / Biztalk.",
      "Support global and regional integration initiatives; provide architectural and technical consultancies to other teams to grow their technical integration competency.",
      "Consult teams on gathering and documenting integration requirements, challenge not optimal solutions.",
      "Create, document and maintain integration technology best practices, standards, templates and examples; collaborate with stakeholders on their implementation.",
      "Perform technical analysis and reviews of existing applications, integrations and define a detailed target cloud application integration design (preferably based on APIs).",
      "Collaborate with our global/affiliates in the review of vendor submissions and preparing recommendations that meet PMI requirements, including compliance with established standards and best practices."
    ],
    "requirements": [
      "3+ years of significant experience in systems integrations technologies area.",
      "Total 5+ years of experience in SAP.",
      "Understanding key processes in technology delivery.",
      "Understanding of enterprise integration practices (SOA, Microservices, API-led, EDA, ESB, Service Mesh, ETL).",
      "Hands-on experience with integration platforms (Mulesoft, Solace, SAP CI, Biztalk or similar).",
      "Experience with SAP CI platform (solutions implementation, platform management) is a must.",
      "Understanding of Integration Design Patterns.",
      "Understanding of API and data security standards.",
      "Hands-on experience with integration standards and technologies (i.e. SOAP, REST, HTTPS, SSL, RESTful).",
      "Experience with DevOps practices.",
      "Strong analytical skills.",
      "Agile practices (SCRUM, Kanban).",
      "Good presentation skills.",
      "Exposure to international environment.",
      "Fluent English."
    ],
    "work_mode": "remote",
    "employment_type": "full-time",
    "seniority": null,
    "description": "Our customer is a multinational corporation with more than a century of history and offices in over 180 countries. Their most ambitious goal at the time is to introduce a range of Reduced-Risk Products (RRPs). The target audience is more than 1 billion of consumers around the globe. IT platform hosts 700+ applications. Intellias mission is to help the client with the engineering of a comprehensive software ecosystem for a game-changing IoT product on the margin of innovative consumer experience and cutting-edge technology. Our teams are involved in the engineering of core platform components for best in class eCommerce, Digital Marketing and IoT solutions.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4346592159&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Hays",
    "position": "OpenSearch DevOps Engineer",
    "location": "Warsaw",
    "salary": "140+ PLN",
    "tech_stack": [
      "OpenSearch",
      "ELK",
      "FluentBit",
      "AWS",
      "Prometheus",
      "Grafana",
      "Ansible",
      "GitLab"
    ],
    "nice_to_have_stack": [],
    "responsibilities": [
      "Development and maintenance of the OpenSearch environment on AWS.",
      "Monitoring applications based on logs (dashboards, alerts, queries).",
      "Optimization of existing solutions.",
      "Integration of new data sources with OpenSearch.",
      "Support for end-users and building effective support."
    ],
    "requirements": [
      "Minimum 3 years of experience with OpenSearch/ELK and FluentBit (administration, development, maintenance).",
      "Practical knowledge of monitoring tools (Prometheus, Grafana or similar).",
      "Strong focus on observability and operational support.",
      "Ability to quickly solve technical problems.",
      "Communication skills and teamwork.",
      "Experience with automation (Ansible, GitLab) is welcome."
    ],
    "work_mode": "hybrid",
    "employment_type": "b2b",
    "seniority": null,
    "description": "Hays IT Contracting focuses on B2B cooperation and matches IT specialists with interesting technological projects in the market. Currently, they are looking for candidates for the OpenSearch DevOps Engineer position.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4352870147&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  },
  {
    "company": "Luxoft",
    "position": "Senior DevOps",
    "location": "Krakow",
    "salary": "",
    "tech_stack": [
      "Linux",
      "Containers",
      "Docker",
      "Kubernetes",
      "Azure",
      "AKS",
      "Vaults",
      "Logs",
      "Terraform",
      "Helm",
      "Bash",
      "Git",
      "GitLab"
    ],
    "nice_to_have_stack": [
      "KAFKA",
      "Hashicorp Vault",
      "Jira",
      "Confluence"
    ],
    "responsibilities": [],
    "requirements": [
      "Experience with Azure Cloud Services and Azure Kubernetes Services.",
      "Hands on experience using the Azure administration portal.",
      "Demonstrable experience deploying enterprise workloads to Azure.",
      "Experience with load balancing and autoscaling on the Cloud.",
      "Experience with Kubernetes container orchestration system.",
      "Experience with concept of Kubernetes Pods and Clusters.",
      "Experience with Docker containers.",
      "Practical experience with Docker containerization and clustering (Kubernetes/AKS/PAS).",
      "Experience with CI/CD practices and tools, such as Jenkins, Git.",
      "Version control system experience (e.g. GitHub).",
      "Experience with APIs, microservices architecture, databases, message queuing etc.",
      "Experience with Unix/Linux server environments.",
      "Understanding of system administration in Linux environments.",
      "Experience with SaaS, logging, monitoring, and troubleshooting.",
      "Experience with configuration management tools (e.g. Ansible, Chef, Puppet).",
      "Self-motivated individual that possesses excellent time management and organizational skills.",
      "An ability to drive to goals and milestones while valuing and maintaining a strong attention to detail.",
      "Clear understanding of application lifecycle development (SDLC).",
      "Full understanding of software development lifecycle best practices.",
      "Excellent troubleshooting abilities."
    ],
    "work_mode": "hybrid",
    "employment_type": "full-time",
    "seniority": "Seniority.senior",
    "description": "Our program was started as the migration of the financial instruments trading legacy mainframe system to a new technical, highly-scalable platform. The success of the program in both the migration and creation of a scalable platform led to it being selected as the strategic platform providing full-scale of advisory services for one of the biggest financial institutions in the world. This has led to significant further investment for legacy system migrations and technical improvements. Currently, we have teams across several locations (Weehawken, Wroclaw, Toronto, Pune) on those projects. Reengineer set of existing applications and introduce additional application interface layer, allowing to query underlying data from third-party applications.",
    "source": "linkedin",
    "url": "https://www.linkedin.com/jobs/search/?currentJobId=4364294246&geoId=105072130&keywords=devops&origin=JOB_COLLECTION_PAGE_SEARCH_BUTTON&refresh=true"
  }
]